{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "f5WxPneDfRjo",
        "outputId": "39ddda85-538a-4455-ad54-ced206d678c7"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "!pip install onnx onnxruntime\n",
        "# !python3 -m pip install mlc-ai -f https://mlc.ai/wheels\n",
        "# !pip install numpy==1.25.2\n",
        "!pip install onnxruntime-gpu\n",
        "# !pip install apache-tvm\n",
        "import onnx\n",
        "import onnxruntime\n",
        "\n",
        "import tvm\n",
        "from tvm import relay\n",
        "from tvm.contrib import graph_executor\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertForQuestionAnswering, BertTokenizerFast\n",
        "from torch.optim import AdamW\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WHIEzRwbiiSv",
        "outputId": "41813e4c-ef6d-440e-edb5-6c48ffcda2d1"
      },
      "outputs": [],
      "source": [
        "tvm.__path__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nXdf_SWdfRjo"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"test_df_sample.csv\")\n",
        "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "question = df.iloc[0][\"question\"]\n",
        "context = df.iloc[0][\"contexts\"]\n",
        "\n",
        "base_inputs = tokenizer(\n",
        "    question,\n",
        "    context,\n",
        "    truncation=True,\n",
        "    padding=\"max_length\",\n",
        "    max_length=128,\n",
        "    return_offsets_mapping=True,\n",
        "    return_tensors=\"pt\",\n",
        ")\n",
        "\n",
        "inputs_cpu = {k: v.clone().detach().to(\"cpu\") for k, v in base_inputs.items()}\n",
        "inputs_gpu = (\n",
        "    {k: v.clone().detach().to(\"cuda\") for k, v in base_inputs.items()}\n",
        "    if torch.cuda.is_available()\n",
        "    else None\n",
        ")\n",
        "\n",
        "valid_keys = {\"input_ids\", \"attention_mask\", \"token_type_ids\"}\n",
        "\n",
        "inputs_cpu = {k: v for k, v in inputs_cpu.items() if k in valid_keys}\n",
        "inputs_gpu = {k: v for k, v in inputs_gpu.items() if k in valid_keys}\n",
        "\n",
        "\n",
        "np_inputs = {k: v.detach().cpu().numpy() for k, v in inputs_gpu.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "id": "KbCqRB5sfRjo",
        "outputId": "baf9d233-034e-4248-f225-c9fb5e4ace05"
      },
      "outputs": [],
      "source": [
        "df.iloc[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_nTkfIYPGIR2"
      },
      "source": [
        "## Inference with Pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Unv9wboOkCe8",
        "outputId": "61c90e60-f8f5-4e02-9ad0-60bc780518ae"
      },
      "outputs": [],
      "source": [
        "model_cpu = BertForQuestionAnswering.from_pretrained(\"bert-base-uncased\")\n",
        "model_cpu.load_state_dict(torch.load(\"bert_qa_model_weights.pt\", map_location=\"cpu\"))\n",
        "model_cpu.eval().to(\"cpu\")\n",
        "\n",
        "# Warm-up + timing on CPU\n",
        "for _ in range(10):\n",
        "    with torch.no_grad():\n",
        "        _ = model_cpu(**inputs_cpu)\n",
        "\n",
        "start = time.time()\n",
        "for _ in range(100):\n",
        "    with torch.no_grad():\n",
        "        _ = model_cpu(**inputs_cpu)\n",
        "end = time.time()\n",
        "print(f\"CPU avg time: {(end - start) * 1000 / 100:.2f} ms\")\n",
        "\n",
        "# Load model on GPU (if available)\n",
        "if torch.cuda.is_available():\n",
        "    model_gpu = BertForQuestionAnswering.from_pretrained(\"bert-base-uncased\")\n",
        "    model_gpu.load_state_dict(\n",
        "        torch.load(\"bert_qa_model_weights.pt\", map_location=\"cuda\")\n",
        "    )\n",
        "    model_gpu.eval().to(\"cuda\")\n",
        "\n",
        "    # Warm-up on GPU\n",
        "    for _ in range(10):\n",
        "        with torch.no_grad():\n",
        "            _ = model_gpu(**inputs_gpu)\n",
        "\n",
        "    torch.cuda.synchronize()  # Ensure GPU is ready\n",
        "    start = time.time()\n",
        "    for _ in range(100):\n",
        "        with torch.no_grad():\n",
        "            _ = model_gpu(**inputs_gpu)\n",
        "    torch.cuda.synchronize()  # Wait for GPU to finish\n",
        "    end = time.time()\n",
        "    print(f\"GPU avg time: {(end - start) * 1000 / 100:.2f} ms\")\n",
        "else:\n",
        "    print(\"GPU not available.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wL-fBoP1GYmg"
      },
      "source": [
        "## Inference with ONNX Runtime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6M_GY0dzLUQM",
        "outputId": "ab7f9c08-a231-469f-c0e0-676d4870e963"
      },
      "outputs": [],
      "source": [
        "import onnxruntime as ort\n",
        "\n",
        "print(ort.get_available_providers())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S7OIp_u7GYXb",
        "outputId": "875c1e84-5cb7-47d6-821f-6712c9e3a4f4"
      },
      "outputs": [],
      "source": [
        "if \"CUDAExecutionProvider\" in ort.get_available_providers():\n",
        "    ort_session = ort.InferenceSession(\n",
        "        \"bert_qa.onnx\", providers=[\"CUDAExecutionProvider\"]\n",
        "    )\n",
        "    print(\"Using GPU for inference (CUDAExecutionProvider).\")\n",
        "else:\n",
        "    ort_session = ort.InferenceSession(\n",
        "        \"bert_qa.onnx\", providers=[\"CPUExecutionProvider\"]\n",
        "    )\n",
        "    print(\"Using CPU for inference.\")\n",
        "\n",
        "start = time.time()\n",
        "for _ in range(100):\n",
        "    ort_outputs = ort_session.run(\n",
        "        None,\n",
        "        {\n",
        "            \"input_ids\": np_inputs[\"input_ids\"],\n",
        "            \"attention_mask\": np_inputs[\"attention_mask\"],\n",
        "            \"token_type_ids\": np_inputs[\"token_type_ids\"],\n",
        "        },\n",
        "    )\n",
        "end = time.time()\n",
        "print(f\"ONNX avg time: {(end - start) * 1000 / 100:.2f} ms\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4XHdJzjfRjp"
      },
      "source": [
        "## Load Baseline Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zeElxka5fRjp",
        "outputId": "1c2dca34-26bf-4339-f7c5-0a1242ac9279"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = BertForQuestionAnswering.from_pretrained(\"bert-base-uncased\")\n",
        "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
        "model.load_state_dict(torch.load(\"bert_qa_model_weights.pt\", map_location=device))\n",
        "model.eval().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u4mzVsiXfRjq"
      },
      "outputs": [],
      "source": [
        "question = df.iloc[0][\"question\"]\n",
        "context = df.iloc[0][\"contexts\"]\n",
        "\n",
        "# inputs = tokenizer(question, context, return_tensors=\"pt\", padding=True)\n",
        "# inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "# input_names = [\"input_ids\", \"attention_mask\", \"token_type_ids\"]\n",
        "# output_names = [\"start_logits\", \"end_logits\"]\n",
        "\n",
        "inputs = tokenizer(\n",
        "    question,\n",
        "    context,\n",
        "    truncation=True,\n",
        "    padding=\"max_length\",\n",
        "    max_length=128,\n",
        "    return_offsets_mapping=True,\n",
        "    return_tensors=\"pt\",\n",
        ")\n",
        "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "input_names = [\"input_ids\", \"attention_mask\", \"token_type_ids\"]\n",
        "output_names = [\"start_logits\", \"end_logits\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "airpKIEdfRjq"
      },
      "source": [
        "##  Export BERT to ONNX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3hdfkU4HfRjq"
      },
      "outputs": [],
      "source": [
        "torch.onnx.export(\n",
        "    model,\n",
        "    (inputs[\"input_ids\"], inputs[\"attention_mask\"], inputs[\"token_type_ids\"]),\n",
        "    \"bert_qa.onnx\",\n",
        "    input_names=input_names,\n",
        "    output_names=output_names,\n",
        "    dynamic_axes={\n",
        "        \"input_ids\": {0: \"batch_size\", 1: \"seq_len\"},\n",
        "        \"attention_mask\": {0: \"batch_size\", 1: \"seq_len\"},\n",
        "        \"token_type_ids\": {0: \"batch_size\", 1: \"seq_len\"},\n",
        "        \"start_logits\": {0: \"batch_size\", 1: \"seq_len\"},\n",
        "        \"end_logits\": {0: \"batch_size\", 1: \"seq_len\"},\n",
        "    },\n",
        "    opset_version=14,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orerj7IBg7Bf"
      },
      "source": [
        "###  Load the ONNX Model into TVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OYZlh_VnjRtX"
      },
      "outputs": [],
      "source": [
        "onnx_model = onnx.load(\"bert_qa.onnx\")\n",
        "\n",
        "batch_size = 1\n",
        "seq_length = 128  # or 384 for long QA\n",
        "\n",
        "input_shapes = {\n",
        "    \"input_ids\": (batch_size, seq_length),\n",
        "    \"attention_mask\": (batch_size, seq_length),\n",
        "    \"token_type_ids\": (batch_size, seq_length),\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DQ-2IhZ3fjgu"
      },
      "outputs": [],
      "source": [
        "mod, params = relay.frontend.from_onnx(onnx_model, shape=input_shapes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XFtwVNy_i-MI"
      },
      "outputs": [],
      "source": [
        "target = \"cuda\" if tvm.cuda().exist else \"llvm\"\n",
        "dev = tvm.device(target, 0)\n",
        "\n",
        "with tvm.transform.PassContext(opt_level=3):\n",
        "    lib = relay.build(mod, target=target, params=params)\n",
        "\n",
        "module = graph_executor.GraphModule(lib[\"default\"](dev))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pmqUTBTIjgYV"
      },
      "outputs": [],
      "source": [
        "# question = \"Who was the president of the US in 2008?\"\n",
        "# context = \"Barack Obama was elected president of the US in 2008.\"\n",
        "\n",
        "inputs = tokenizer(\n",
        "    question,\n",
        "    context,\n",
        "    return_tensors=\"np\",\n",
        "    padding=\"max_length\",\n",
        "    max_length=seq_length,\n",
        "    truncation=True,\n",
        ")\n",
        "\n",
        "# Set inputs\n",
        "module.set_input(\"input_ids\", inputs[\"input_ids\"])\n",
        "module.set_input(\"attention_mask\", inputs[\"attention_mask\"])\n",
        "module.set_input(\"token_type_ids\", inputs[\"token_type_ids\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uxSTEy0Gj0I8"
      },
      "outputs": [],
      "source": [
        "module.run()\n",
        "\n",
        "start_logits = module.get_output(0).numpy()\n",
        "end_logits = module.get_output(1).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BP0r4uQyj7-l",
        "outputId": "4bb1d202-41ce-40cb-c68c-b3680c2cefb0"
      },
      "outputs": [],
      "source": [
        "start = np.argmax(start_logits)\n",
        "end = np.argmax(end_logits)\n",
        "\n",
        "tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
        "answer = tokenizer.convert_tokens_to_string(tokens[start : end + 1])\n",
        "\n",
        "print(\"Answer:\", answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A4pCjPxNj-xl"
      },
      "outputs": [],
      "source": [
        "lib.export_library(\"bert_qa_tvm.so\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPokTV6xGfFd"
      },
      "source": [
        "## Inference with TVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x7JTlaKfVN6u"
      },
      "outputs": [],
      "source": [
        "# target = tvm.target.Target(\"cuda -arch=sm_75\")\n",
        "# target = 'cuda -arch=sm_75'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rhDy44NFGHsC"
      },
      "outputs": [],
      "source": [
        "# onnx_model = onnx.load(\"bert_qa.onnx\")\n",
        "# input_shape = {\n",
        "#     \"input_ids\": np_inputs[\"input_ids\"].shape,\n",
        "#     \"attention_mask\": np_inputs[\"attention_mask\"].shape,\n",
        "#     \"token_type_ids\": np_inputs[\"token_type_ids\"].shape\n",
        "# }\n",
        "# mod, params = relay.frontend.from_onnx(onnx_model, shape=input_shape)\n",
        "\n",
        "target = \"cuda\" if tvm.cuda().exist else \"llvm\"\n",
        "dev = tvm.device(target, 0)\n",
        "\n",
        "# # Load the compiled module\n",
        "loaded_lib = tvm.runtime.load_module(\"bert_qa_tvm.so\")\n",
        "\n",
        "# print(\"input_ids shape:\", np_inputs[\"input_ids\"].shape)\n",
        "# print(\"attention_mask shape:\", np_inputs[\"attention_mask\"].shape)\n",
        "# print(\"token_type_ids shape:\", np_inputs[\"token_type_ids\"].shape)\n",
        "\n",
        "\n",
        "# with tvm.transform.PassContext(opt_level=3, required_pass=[\"FastMath\"]):\n",
        "# lib = relay.build(mod, target=target, params=params)\n",
        "\n",
        "module = graph_executor.GraphModule(loaded_lib[\"default\"](dev))\n",
        "module.set_input(\"input_ids\", tvm.nd.array(np_inputs[\"input_ids\"].astype(\"int64\")))\n",
        "module.set_input(\n",
        "    \"attention_mask\", tvm.nd.array(np_inputs[\"attention_mask\"].astype(\"int64\"))\n",
        ")\n",
        "module.set_input(\n",
        "    \"token_type_ids\", tvm.nd.array(np_inputs[\"token_type_ids\"].astype(\"int64\"))\n",
        ")\n",
        "\n",
        "# lib.export_library(\"bert_qa_tvm.so\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Q1tqWAWP8KE",
        "outputId": "9dde1751-86f1-47a1-8efa-0ef7ef2745ad"
      },
      "outputs": [],
      "source": [
        "start = time.time()\n",
        "for _ in range(100):\n",
        "    module.run()\n",
        "    start_logits = module.get_output(0).numpy()\n",
        "    end_logits = module.get_output(1).numpy()\n",
        "end = time.time()\n",
        "print(f\"TVM avg time: {(end - start) * 1000 / 100:.2f} ms\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mE59YaWMTP6Y",
        "outputId": "0d6ec579-7745-46c1-feb1-c9f291b5a035"
      },
      "outputs": [],
      "source": [
        "tvm.cuda().exist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wuf-H2lPT2-V",
        "outputId": "bdcabae8-dc1c-4e52-f177-c014abb7f848"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "print(torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iKF2aWO00ul5",
        "outputId": "0686c543-4c36-4247-87db-10868440ea4e"
      },
      "outputs": [],
      "source": [
        "%cd /content\n",
        "!git clone --recursive https://github.com/apache/tvm tvm\n",
        "%cd tvm\n",
        "!mkdir build\n",
        "!cp cmake/config.cmake build/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LV6ADL2MVXCB"
      },
      "outputs": [],
      "source": [
        "!sed -i 's/USE_LLVM OFF/USE_LLVM ON/' build/config.cmake\n",
        "!sed -i 's/USE_CUDA OFF/USE_CUDA ON/' build/config.cmake"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HaArPQS_VbKx",
        "outputId": "118fd252-a806-41a1-bebf-60e8103ddf62"
      },
      "outputs": [],
      "source": [
        "%cd build\n",
        "!cmake ..\n",
        "!make -j4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jcd2mX8gV4uk",
        "outputId": "13662a08-1af3-4c34-a11c-c4a80745a4f2"
      },
      "outputs": [],
      "source": [
        "%cd ../python\n",
        "!pip install -e ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 361
        },
        "id": "Q_niL5CpiESR",
        "outputId": "4960b958-0a55-41f3-a2e1-f3dabe3e099f"
      },
      "outputs": [],
      "source": [
        "import tvm\n",
        "from tvm import relay\n",
        "\n",
        "print(\"TVM version:\", tvm.__version__)\n",
        "print(\"CUDA available:\", tvm.cuda().exist)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ozLicOkt8_u",
        "outputId": "1668c2f6-157b-4f51-dfdb-6f3514c068be"
      },
      "outputs": [],
      "source": [
        "!ls /content/tvm/build"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ktRw-rwTuOl5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"TVM_HOME\"] = \"/content/tvm\"\n",
        "os.environ[\"PYTHONPATH\"] = \"/content/tvm/python\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TIJK90kzxYOs"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
